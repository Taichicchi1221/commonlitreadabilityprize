# ====================================================
# config
# ====================================================
log:
  mlflow:
    save_dir: ../mlruns
    experiment_name: experiments

general:
  seed: 16
  debug: False

dir:
  work_dir: /workspaces/commonlitreadabilityprize/work
  input_dir: ../input/commonlitreadabilityprize

training:
  splitter: StratifiedKFold # [KFold, StratifiedKFold]
  n_fold: 5
  precision: 16
  stochastic_weight_avg: False
  epochs: 15
  accumulate_grad_batches: 8
  steps_per_epoch: -1 # must be set in script

model:
  name: roberta-large
  hidden_features: 512 # hidden features of Attention
  multi_dropout_rate: 0.5
  multi_dropout_num: 5
  freeze_embeddings: True
  params:
    hidden_dropout_prob: 0.00
    layer_norm_eps: 1.0e-07

loader:
  train:
    batch_size: 1
    shuffle: True
    drop_last: True
    pin_memory: False
    num_workers: 0
  test:
    batch_size: 32
    shuffle: False
    drop_last: False
    pin_memory: False
    num_workers: 0

tokenizer:
  name: ${model.name}
  max_length: 314

loss:
  name: MSELoss # [MSELoss, SmoothL1Loss]
  params: 
    reduction: mean
    # beta: 0.50 # SmoothL1Loss

optimizer:
  name: Lamb # [SGD, Adam, AdamW, Lamb]
  params:
    lr: 1.0e-08
    weight_decay: 0.0 # AdamW, Lamb
    adam: True # Lamb
    betas: [0.9, 0.999] # Lamb
    eps: 1.0e-06 # Lamb
    debias: False # Lamb

scheduler:
  name: OneCycleLR # [None, ReduceLROnPlateau, CosineAnealingLR, CosineAnnealingWarmRestarts, OneCycleLR]
  interval: step # [epoch, step]
  params:
    # mode: min # ReduceLROnPlateau
    # factor: 0.2 # ReduceLROnPlateau
    # patience: 2 # ReduceLROnPlateau
    # eps: 1.0e-06 # ReduceLROnPlateau
    # T_max: ${TRAIN.epochs} # CosineAnealingLR
    # eta_min: 1.0e-8 # CosineAnealingLR, CosineAnnealingWarmRestarts
    # T_0: ${training.steps_per_epoch} # CosineAnnealingWarmRestarts
    # T_mult: 2 # CosineAnnealingWarmRestarts
    # eta_min: 1.0e-8 # CosineAnealingLR, CosineAnnealingWarmRestarts

    max_lr: 2.0e-05 # OneCycleLR
    pct_start: 0.2 # OneCycleLR
    steps_per_epoch: ${training.steps_per_epoch} # OneCycleLR
    epochs: ${training.epochs} # OneCycleLR
    verbose: False
