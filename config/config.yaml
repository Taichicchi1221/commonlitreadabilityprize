# ====================================================
# config
# ====================================================
log:
  mlflow:
    save_dir: ../mlruns
    experiment_name: baseline

general:
  seed: 6
  debug: False

dir:
  work_dir: /workspaces/commonlitreadabilityprize/work
  input_dir: ../input/commonlitreadabilityprize

training:
  splitter: StratifiedKFold # [KFold, StratifiedKFold]
  n_fold: 5
  precision: 16
  stochastic_weight_avg: False
  epochs: 50
  accumulate_grad_batches: 1
  steps_per_epoch: -1 # must be set in script

model:
  name: roberta-base
  multisample_dropout: 5
  multisample_dropout_rate: 0.5
  params:
    hidden_dropout_prob: 0.00
    layer_norm_eps: 1.0e-07

loader:
  train:
    batch_size: 8
    shuffle: True
    drop_last: True
    pin_memory: False
    num_workers: 0
  test:
    batch_size: 32
    shuffle: False
    drop_last: False
    pin_memory: False
    num_workers: 0

tokenizer:
  name: ${model.name}
  max_length: 314

loss:
  name: SmoothL1Loss # [MSELoss, SmoothL1Loss]
  params: 
    reduction: mean
    beta: 0.25 # SmoothL1Loss

optimizer:
  name: AdamW # [SGD, Adam, AdamW]
  params:
    lr: 1.0e-16
    weight_decay: 1.0e-02 # AdamW

scheduler:
  name: OneCycleLR # [ReduceLROnPlateau, CosineAnealingLR, CosineAnnealingWarmRestarts, OneCycleLR]
  interval: step # [epoch, step]
  params:
    # mode: min # ReduceLROnPlateau
    # factor: 0.2 # ReduceLROnPlateau
    # patience: 4 # ReduceLROnPlateau
    # eps: 1.0e-06 # ReduceLROnPlateau
    # T_max: ${TRAIN.epochs} # CosineAnealingLR
    # eta_min: 1.0e-8 # CosineAnealingLR, CosineAnnealingWarmRestarts
    # T_0: ${training.steps_per_epoch} # CosineAnnealingWarmRestarts
    # T_mult: 2 # CosineAnnealingWarmRestarts
    # eta_min: 1.0e-8 # CosineAnealingLR, CosineAnnealingWarmRestarts

    max_lr: 2.0e-05 # OneCycleLR
    pct_start: 0.2 # OneCycleLR
    steps_per_epoch: ${training.steps_per_epoch} # OneCycleLR
    epochs: ${training.epochs} # OneCycleLR
    verbose: False
