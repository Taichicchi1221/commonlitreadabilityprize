# ====================================================
# config
# ====================================================
log:
  mlflow:
    save_dir: ../mlruns
    experiment_name: baseline

general:
  seed: 0
  debug: False

dir:
  work_dir: /workspaces/commonlitreadabilityprize/work
  input_dir: ../input/commonlitreadabilityprize

training:
  n_fold: 5
  shuffle_seed: 0
  precision: 16
  stochastic_weight_avg: False
  epochs: 30
  steps_per_epoch: -1 # must be set in script

model:
  name: roberta-base
  params:
    hidden_dropout_prob: 0.0

loader:
  train:
    batch_size: 8
    shuffle: True
    drop_last: True
    pin_memory: False
  test:
    batch_size: 32
    shuffle: False
    drop_last: False
    pin_memory: False

tokenizer:
  max_length: 512

optimizer:
  name: Adam # [SGD, Adam, AdamW]
  params:
    lr: 1.0e-08
    # weight_decay: 1.0e-02 # AdamW

scheduler:
  name: OneCycleLR # [ReduceLROnPlateau, CosineAnealingLR, CosineAnnealingWarmRestarts, OneCycleLR]
  interval: step # [epoch, step]
  params:
    # mode: min # ReduceLROnPlateau
    # factor: 0.2 # ReduceLROnPlateau
    # patience: 4 # ReduceLROnPlateau
    # eps: 1.0e-06 # ReduceLROnPlateau
    # T_max: ${TRAIN.epochs} # CosineAnealingLR
    # eta_min: 1.0e-8 # 
    # T_0: ${training.steps_per_epoch} # CosineAnnealingWarmRestarts
    # T_mult: 2 # CosineAnnealingWarmRestarts
    # eta_min: 1.0e-8 # CosineAnealingLR, CosineAnnealingWarmRestarts
    max_lr: 2.0e-05 # OneCycleLR
    pct_start: 0.1 # OneCycleLR
    steps_per_epoch: ${training.steps_per_epoch} # OneCycleLR
    epochs: ${training.epochs} # OneCycleLR
    verbose: False
